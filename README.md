# Decision Tree in Machine Learning â€“ Key Concepts

## 1. Decision Tree
A supervised learning algorithm that splits data into branches based on feature values, forming a tree-like structure to make predictions.

- **Classification Tree** â†’ predicts categorical values.  
- **Regression Tree** â†’ predicts continuous values.

     â”Œâ”€â”€ Feature A? â”€â”€â”
     â”‚               â”‚
  Yesâ–¼               â–¼No


---

## 2. Entropy
A measure of **impurity or randomness** in the dataset.

**Formula:**
Entropy(S) = -Î£ p_i * logâ‚‚(p_i)
where `p_i` = proportion of class *i*.

- **High entropy** â†’ data is more mixed.  
- **Low entropy** â†’ data is more pure.

---

## 3. Gini Impurity
Another measure of impurity used in CART (Classification and Regression Trees).

**Formula:**
Gini(S) = 1 - Î£ (p_i)Â²
- **Gini = 0** â†’ perfectly pure node.  
- Faster to compute than entropy.

---

## 4. Information Gain
The **reduction in impurity** after a split.

**Formula (using entropy):**
IG = Entropy(Parent) âˆ’ Î£ ( |Child_k| / |Parent| ) Ã— Entropy(Child_k)
- Higher IG â†’ better split.

---

## 5. Pre-Pruning (Early Stopping)
Stop growing the tree early to avoid overfitting.

**Methods:**
- Set maximum depth.  
- Minimum samples per split.  
- Minimum information gain threshold.

---

## 6. Post-Pruning
Grow the full tree first, then remove branches that have little impact.

**Methods:**
- Cost complexity pruning (**CARTâ€™s alpha pruning**).  
- Reduced error pruning.

---

## ğŸ“Š Summary Table

| Term               | Meaning                              | Goal                  |
|--------------------|--------------------------------------|-----------------------|
| **Entropy**        | Impurity measure (log-based)         | Find best split       |
| **Gini Impurity**  | Impurity measure (probability-based) | Find best split       |
| **Information Gain** | Reduction in impurity              | Choose split feature  |
| **Pre-Pruning**    | Stop early                           | Prevent overfitting   |
| **Post-Pruning**   | Prune after training                 | Reduce overfitting    |

---

## ğŸŒ³ Pruning Workflow
[Train full tree]
â†“
[Evaluate branches]
â†“
[Remove low-impact splits]
â†“
[Smaller tree â†’ Less overfitting]
