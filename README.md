Decision Tree in Machine Learning â€“ Key Concepts
1. Decision Tree
A supervised learning algorithm that splits data into branches based on feature values, forming a tree-like structure to make predictions.

Classification Tree â†’ predicts categorical values.

Regression Tree â†’ predicts continuous values.

2. Entropy
A measure of impurity or randomness in the dataset.

Formula:

ğ¸
ğ‘›
ğ‘¡
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘¦
(
ğ‘†
)
=
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘
ğ‘–
log
â¡
2
ğ‘
ğ‘–
Entropy(S)=âˆ’ 
i=1
âˆ‘
n
â€‹
 p 
i
â€‹
 log 
2
â€‹
 p 
i
â€‹
 
where 
ğ‘
ğ‘–
p 
i
â€‹
  = proportion of class 
ğ‘–
i.

High entropy â†’ data is more mixed.

Low entropy â†’ data is more pure.

3. Gini Impurity
Another measure of impurity used in CART (Classification and Regression Trees).

Formula:

ğº
ğ‘–
ğ‘›
ğ‘–
(
ğ‘†
)
=
1
âˆ’
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘
ğ‘–
2
Gini(S)=1âˆ’ 
i=1
âˆ‘
n
â€‹
 p 
i
2
â€‹
 
Gini = 0 â†’ perfectly pure node.

Faster to compute than entropy.

4. Information Gain
The reduction in impurity after a split.

Formula (using entropy):

ğ¼
ğº
=
ğ¸
ğ‘›
ğ‘¡
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘¦
(
ğ‘ƒ
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘›
ğ‘¡
)
âˆ’
âˆ‘
ğ‘˜
âˆ£
ğ¶
â„
ğ‘–
ğ‘™
ğ‘‘
ğ‘˜
âˆ£
âˆ£
ğ‘ƒ
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘›
ğ‘¡
âˆ£
â‹…
ğ¸
ğ‘›
ğ‘¡
ğ‘Ÿ
ğ‘œ
ğ‘
ğ‘¦
(
ğ¶
â„
ğ‘–
ğ‘™
ğ‘‘
ğ‘˜
)
IG=Entropy(Parent)âˆ’ 
k
âˆ‘
â€‹
  
âˆ£Parentâˆ£
âˆ£Child 
k
â€‹
 âˆ£
â€‹
 â‹…Entropy(Child 
k
â€‹
 )
Higher IG â†’ better split.

5. Pre-Pruning (Early Stopping)
Stop growing the tree early to avoid overfitting.

Methods:

Set max depth.

Minimum samples per split.

Minimum information gain threshold.

6. Post-Pruning
Grow the full tree first, then remove branches that have little impact.

Methods:

Cost complexity pruning (CARTâ€™s alpha pruning).

Reduced error pruning.

âœ… Summary Table:

Term	Meaning	Goal
Entropy	Impurity measure (log-based)	Find best split
Gini Impurity	Impurity measure (probability-based)	Find best split
Information Gain	Reduction in impurity	Choose split feature
Pre-Pruning	Stop early	Prevent overfitting
Post-Pruning	Prune after training	Reduce overfitting

